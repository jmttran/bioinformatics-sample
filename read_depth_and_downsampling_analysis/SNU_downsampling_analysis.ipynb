{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5813d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c2b0739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample_results = pd.read_csv(\"./SNU_downsample_results/results_table01.tsv\", sep = \" \")\n",
    "# original_results = pd.read_csv(\"./SNU_downsample_results/results_table_original.tsv\", sep = \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5245f345",
   "metadata": {},
   "source": [
    "# helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6fcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(results_df):\n",
    "    start_list = [str(d) for d in list(results_df[\"start\"])]\n",
    "    end_list = [str(d) for d in list(results_df[\"end\"])]\n",
    "    results_df[\"bin\"] = results_df[\"seq\"] + start_list + end_list\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d67547bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_subsets(downsample_path, original_path):\n",
    "    downsample_results = pd.read_csv(downsample_path, sep = \" \")\n",
    "    original_results = pd.read_csv(original_path, sep = \" \")\n",
    "\n",
    "    # add bin labels\n",
    "    downsample_results = create_bins(downsample_results)\n",
    "    original_results = create_bins(original_results)\n",
    "\n",
    "    # get subset of bins + cells that should be kept\n",
    "    bin_subset = set(list(downsample_results[\"bin\"])).intersection(set(list(original_results[\"bin\"])))\n",
    "    cell_subset = set(list((downsample_results.columns))).intersection(set(list(original_results.columns)))\n",
    "\n",
    "    # subset both\n",
    "    downsample_subset = downsample_results[downsample_results.columns[downsample_results.columns.isin(cell_subset)]].loc[downsample_results[\"bin\"].isin(bin_subset)].reset_index(drop=True)\n",
    "    original_subset = original_results[original_results.columns[original_results.columns.isin(cell_subset)]].loc[original_results[\"bin\"].isin(bin_subset)].reset_index(drop=True)\n",
    "\n",
    "    # make sure column names match\n",
    "    downsample_subset = downsample_subset[original_subset.columns.tolist()]\n",
    "    \n",
    "    return[downsample_subset, original_subset, original_results]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c315251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics(ds_subset, og_subset, original_results):\n",
    "    \n",
    "    ds_call_array = np.array(ds_subset.iloc[:,3:-1])\n",
    "    og_call_array = np.array(og_subset.iloc[:,3:-1])\n",
    "    original_results_call_array = np.array(original_results.iloc[:,3:-1])\n",
    "    \n",
    "    # all calls labeled normal\n",
    "    original_normal = len(np.where((original_results_call_array == 1)[0]))\n",
    "    TP_normal = len(np.where((og_call_array == 1) & (ds_call_array == 1))[0]) # both called\n",
    "    FP_normal = len(np.where((og_call_array != 1) & (ds_call_array == 1))[0]) # subset called normal; original did not\n",
    "    FN_normal = len(np.where((og_call_array == 1) & (ds_call_array != 1))[0]) # subset did not call normal; original did\n",
    "    TN_normal = len(np.where((og_call_array != 1) & (ds_call_array != 1))[0]) # both said was not normal\n",
    "\n",
    "#     print(TP_normal)\n",
    "#     print(FP_normal)\n",
    "#     print(FN_normal)\n",
    "#     print(TN_normal)\n",
    "    \n",
    "    percent_normal = TP_normal/original_normal\n",
    "    precision_normal = TP_normal/(TP_normal+FP_normal)\n",
    "    recall_normal = TP_normal/(TP_normal+FN_normal)\n",
    "    F1_normal = 2*((precision_normal*recall_normal)/(precision_normal+recall_normal))\n",
    "    specificity_normal = TN_normal/(TN_normal+FP_normal)\n",
    "    \n",
    "    # all calls labeled loss\n",
    "    original_loss = len(np.where((original_results_call_array == 0)[0]))\n",
    "    TP_loss = len(np.where((og_call_array == 0) & (ds_call_array == 0))[0]) # both called\n",
    "    FP_loss = len(np.where((og_call_array != 0) & (ds_call_array == 0))[0]) # subset called loss; original did not\n",
    "    FN_loss = len(np.where((og_call_array == 0) & (ds_call_array != 0))[0]) # subset did not call loss; original did\n",
    "    TN_loss = len(np.where((og_call_array != 0) & (ds_call_array != 0))[0]) # both said was not loss\n",
    "\n",
    "    percent_loss = TP_loss/original_loss\n",
    "    precision_loss = TP_loss/(TP_loss+FP_loss)\n",
    "    recall_loss = TP_loss/(TP_loss+FN_loss)\n",
    "    F1_loss = 2*((precision_loss*recall_loss)/(precision_loss+recall_loss))\n",
    "    specificity_loss = TN_loss/(TN_loss+FP_loss)\n",
    "    \n",
    "    # all calls labeled gain \n",
    "    original_gain = len(np.where((original_results_call_array > 1)[0]))\n",
    "    TP_gain = len(np.where((og_call_array > 1) & (og_call_array == ds_call_array))[0]) # both called\n",
    "    FP_gain = len(np.where((og_call_array <= 1) & (ds_call_array > 1))[0]) # subset called gain; original did not\n",
    "    FN_gain = len(np.where((og_call_array > 1) & (ds_call_array <= 1))[0]) # subset did not call gain; original did\n",
    "    TN_gain = len(np.where((og_call_array <= 1) & (ds_call_array <= 1))[0]) # both said was not gain\n",
    "\n",
    "    percent_gain = TP_gain/original_gain\n",
    "    precision_gain = TP_gain/(TP_gain+FP_gain)\n",
    "    recall_gain = TP_gain/(TP_gain+FN_gain)\n",
    "    F1_gain = 2*((precision_gain*recall_gain)/(precision_gain+recall_gain))\n",
    "    specificity_gain = TN_gain/(TN_gain+FP_gain)\n",
    "    \n",
    "    statistic_dictionary = {}\n",
    "    statistic_dictionary[\"loss\"] = {\"percent_call\": percent_loss, \"precision\": precision_loss, \"recall\": recall_loss, \"specificity\": specificity_loss, \"F1\": F1_loss}\n",
    "    statistic_dictionary[\"normal\"] = {\"percent_call\": percent_normal, \"precision\": precision_normal, \"recall\": recall_normal, \"specificity\": specificity_normal,\"F1\": F1_normal}\n",
    "    statistic_dictionary[\"gain\"] = {\"percent_call\": percent_gain, \"precision\": precision_gain, \"recall\": recall_gain, \"specificity\": specificity_gain, \"F1\": F1_gain}\n",
    "\n",
    "    return statistic_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11424788",
   "metadata": {},
   "source": [
    "# get stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ebd3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = \"./SNU_downsample_results/results_table01.tsv\"\n",
    "og_path = \"./SNU_downsample_results/results_table_original.tsv\"\n",
    "\n",
    "# subset_dfs = create_df_subsets(ds_path, og_path)\n",
    "# ds_subset = subset_dfs[0]\n",
    "# og_subset = subset_dfs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56ca4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "(2618, 118)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 11%|█████                                        | 1/9 [00:09<01:17,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(5554, 428)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 22%|██████████                                   | 2/9 [00:15<00:49,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(7621, 1221)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|███████████████                              | 3/9 [00:24<00:47,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(9534, 1543)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 44%|████████████████████                         | 4/9 [00:40<00:56, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(11036, 1774)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 56%|█████████████████████████                    | 5/9 [00:57<00:54, 13.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(12343, 1918)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|██████████████████████████████               | 6/9 [01:17<00:46, 15.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(13258, 1937)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 78%|███████████████████████████████████          | 7/9 [01:42<00:37, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(14320, 1715)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 89%|████████████████████████████████████████     | 8/9 [02:04<00:19, 19.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(15640, 1397)\n",
      "(25976, 2467)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [02:30<00:00, 16.71s/it]\n"
     ]
    }
   ],
   "source": [
    "statistic_dict = {}\n",
    "statistic_df = pd.DataFrame(columns=[\"downsample_frac\", \"mean_depth\", \"event\", \"percent_call\", \"precision\", \"recall\", \"specificity\", \"F1\"])\n",
    "\n",
    "for i in tqdm(range(1,10)):\n",
    "    \n",
    "#for i in range(1,2):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # get read depth\n",
    "    ds_fragment_counts_path = f\"./epiAneufinder/SNU_downsample_files/fragment_files/fragments_filtered_00{i}.tsv\"\n",
    "    fragments_counts = pd.read_csv(ds_fragment_counts_path, sep = \"\\t\", header=None)\n",
    "    \n",
    "    mean_depth = (fragments_counts).groupby([3]).size().mean()\n",
    "    \n",
    "    # get read statistics\n",
    "    ds_path = f\"./SNU_downsample_files/2k_results/results_table_00{i}.tsv\"\n",
    "    og_path = \"./SNU_downsample_files/2k_results/results_table_original.tsv\"\n",
    "    \n",
    "    subset_dfs = create_df_subsets(ds_path, og_path)\n",
    "    ds_subset = subset_dfs[0]\n",
    "    og_subset = subset_dfs[1]\n",
    "    original_results = subset_dfs[2]\n",
    "    \n",
    "    print(og_subset.shape)\n",
    "    print(original_results.shape)\n",
    "    \n",
    "    statistic_dict[f\"0.0{i}\"] = get_statistics(ds_subset, og_subset, original_results)\n",
    "    \n",
    "    statistic_df.loc[len(statistic_df.index)] = [f\"0.0{i}\", \n",
    "                                                 mean_depth,\n",
    "                                                 \"loss\", \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"loss\"][\"percent_call\"],\n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"loss\"][\"precision\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"loss\"][\"recall\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"loss\"][\"specificity\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"loss\"][\"F1\"]]\n",
    "                                                 \n",
    "    statistic_df.loc[len(statistic_df.index)] = [f\"0.0{i}\", \n",
    "                                                 mean_depth,\n",
    "                                                 \"normal\", \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"normal\"][\"percent_call\"],\n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"normal\"][\"precision\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"normal\"][\"recall\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"normal\"][\"specificity\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"normal\"][\"F1\"]]\n",
    "                                                 \n",
    "    statistic_df.loc[len(statistic_df.index)] = [f\"0.0{i}\", \n",
    "                                                 mean_depth,\n",
    "                                                 \"gain\", \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"gain\"][\"percent_call\"],\n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"gain\"][\"precision\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"gain\"][\"recall\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"gain\"][\"specificity\"], \n",
    "                                                 statistic_dict[f\"0.0{i}\"][\"gain\"][\"F1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4673e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_df.to_csv(\"downsample_statistics_00x_REDO.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
